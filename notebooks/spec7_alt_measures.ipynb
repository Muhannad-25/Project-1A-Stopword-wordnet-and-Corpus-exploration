{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49156a02-3d45-4e23-817b-f477775c45be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents (sentences): 23600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Measure</th>\n",
       "      <th>Overlap</th>\n",
       "      <th>TopN</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Ratio %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frequency</td>\n",
       "      <td>91</td>\n",
       "      <td>198</td>\n",
       "      <td>0.459596</td>\n",
       "      <td>45.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDF</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>92</td>\n",
       "      <td>198</td>\n",
       "      <td>0.464646</td>\n",
       "      <td>46.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Entropy</td>\n",
       "      <td>91</td>\n",
       "      <td>198</td>\n",
       "      <td>0.459596</td>\n",
       "      <td>45.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Info_Content</td>\n",
       "      <td>91</td>\n",
       "      <td>198</td>\n",
       "      <td>0.459596</td>\n",
       "      <td>45.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Info_Gain</td>\n",
       "      <td>40</td>\n",
       "      <td>198</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>20.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KL_Divergence</td>\n",
       "      <td>91</td>\n",
       "      <td>198</td>\n",
       "      <td>0.459596</td>\n",
       "      <td>45.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Measure  Overlap  TopN     Ratio  Ratio %\n",
       "0      Frequency       91   198  0.459596    45.96\n",
       "1            IDF        0   198  0.000000     0.00\n",
       "2         TF-IDF       92   198  0.464646    46.46\n",
       "3        Entropy       91   198  0.459596    45.96\n",
       "4   Info_Content       91   198  0.459596    45.96\n",
       "5      Info_Gain       40   198  0.202020    20.20\n",
       "6  KL_Divergence       91   198  0.459596    45.96"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved (relative):\n",
      "- artifacts/spec7_alternative_measures.csv\n"
     ]
    }
   ],
   "source": [
    "# Spec 7 â€” Alternative Measures for Stopword Identification\n",
    "# Based on: Sarica & Luo (2021) \"Stopwords in technical language processing\"\n",
    "# Test IDF, TF-IDF, Entropy, Information Content, Information Gain, KL Divergence\n",
    "# Compare each measure to NLTK stopword list (|L| words)\n",
    "\n",
    "import os, math, nltk, numpy as np, pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords, webtext, gutenberg\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- Setup ----------\n",
    "ART_DIR = os.path.abspath(\"../artifacts\" if os.path.basename(os.getcwd())==\"notebooks\" else \"./artifacts\")\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('webtext', quiet=True)\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def clean_words(text):\n",
    "    \"\"\"Return lowercase alphabetic tokens.\"\"\"\n",
    "    toks = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    return toks\n",
    "\n",
    "def make_sentences(corpus_words):\n",
    "    \"\"\"Treat each sentence as a 'document'.\"\"\"\n",
    "    text = \" \".join(corpus_words)\n",
    "    sents = sent_tokenize(text)\n",
    "    docs = [\" \".join(clean_words(s)) for s in sents if len(s.split()) > 2]\n",
    "    return docs\n",
    "\n",
    "def entropy(prob_list):\n",
    "    \"\"\"Shannon entropy (base e).\"\"\"\n",
    "    return -sum(p * math.log(p + 1e-12) for p in prob_list)\n",
    "\n",
    "# ---------- Load corpus ----------\n",
    "corpus = []\n",
    "for fid in webtext.fileids()[:5]:\n",
    "    corpus.extend(webtext.words(fid))\n",
    "if len(corpus) < 20000:  # fallback\n",
    "    for fid in gutenberg.fileids()[:2]:\n",
    "        corpus.extend(gutenberg.words(fid))\n",
    "\n",
    "docs = make_sentences(corpus)\n",
    "print(f\"Documents (sentences): {len(docs)}\")\n",
    "\n",
    "# ---------- Stopword list ----------\n",
    "L = set(stopwords.words('english'))\n",
    "TopN = len(L)\n",
    "\n",
    "# ---------- Frequency (baseline) ----------\n",
    "freq = Counter(w for d in docs for w in d.split())\n",
    "top_freq = [w for w, _ in freq.most_common(TopN)]\n",
    "overlap_freq = len([w for w in top_freq if w in L])\n",
    "\n",
    "# ---------- IDF & TF-IDF ----------\n",
    "tfidf_vec = TfidfVectorizer(token_pattern=r\"\\b[a-zA-Z]+\\b\", lowercase=True)\n",
    "tfidf = tfidf_vec.fit_transform(docs)\n",
    "idf_scores = dict(zip(tfidf_vec.get_feature_names_out(), tfidf_vec.idf_))\n",
    "tfidf_scores = np.asarray(tfidf.mean(axis=0)).flatten()\n",
    "tfidf_scores = dict(zip(tfidf_vec.get_feature_names_out(), tfidf_scores))\n",
    "\n",
    "top_idf = sorted(idf_scores, key=idf_scores.get, reverse=True)[:TopN]\n",
    "top_tfidf = sorted(tfidf_scores, key=tfidf_scores.get, reverse=True)[:TopN]\n",
    "overlap_idf = len([w for w in top_idf if w in L])\n",
    "overlap_tfidf = len([w for w in top_tfidf if w in L])\n",
    "\n",
    "# ---------- Entropy ----------\n",
    "word_doc_counts = defaultdict(lambda: np.zeros(len(docs)))\n",
    "for i, d in enumerate(docs):\n",
    "    for w in set(d.split()):\n",
    "        word_doc_counts[w][i] = 1\n",
    "entropy_scores = {w: entropy(v / v.sum()) if v.sum() > 0 else 0 for w, v in word_doc_counts.items()}\n",
    "top_entropy = sorted(entropy_scores, key=entropy_scores.get, reverse=True)[:TopN]\n",
    "overlap_entropy = len([w for w in top_entropy if w in L])\n",
    "\n",
    "# ---------- Information Content ----------\n",
    "total_tokens = sum(freq.values())\n",
    "info_content = {w: -math.log(freq[w]/total_tokens + 1e-12) for w in freq}\n",
    "top_ic = sorted(info_content, key=info_content.get)[:TopN]  # lowest = most common\n",
    "overlap_ic = len([w for w in top_ic if w in L])\n",
    "\n",
    "# ---------- Information Gain (approx) ----------\n",
    "# simplified version: difference in entropy when removing a word\n",
    "H_total = entropy(np.array(list(freq.values())) / total_tokens)\n",
    "IG_scores = {}\n",
    "for w in freq:\n",
    "    p_w = freq[w] / total_tokens\n",
    "    if p_w == 0: continue\n",
    "    freq_wo = freq.copy()\n",
    "    del freq_wo[w]\n",
    "    total_wo = sum(freq_wo.values())\n",
    "    H_wo = entropy(np.array(list(freq_wo.values())) / total_wo)\n",
    "    IG_scores[w] = H_total - H_wo\n",
    "top_ig = sorted(IG_scores, key=IG_scores.get, reverse=True)[:TopN]\n",
    "overlap_ig = len([w for w in top_ig if w in L])\n",
    "\n",
    "# ---------- Kullback-Leibler Divergence ----------\n",
    "# measure uniformity of word distribution across docs\n",
    "KL_scores = {}\n",
    "for w, v in word_doc_counts.items():\n",
    "    p = v / (v.sum() + 1e-12)\n",
    "    uniform = np.ones(len(p)) / len(p)\n",
    "    KL = np.sum(p * np.log((p + 1e-12) / (uniform + 1e-12)))\n",
    "    KL_scores[w] = KL\n",
    "top_kl = sorted(KL_scores, key=KL_scores.get)[:TopN]  # smaller KL = more uniform\n",
    "overlap_kl = len([w for w in top_kl if w in L])\n",
    "\n",
    "# ---------- Results summary ----------\n",
    "results = pd.DataFrame([\n",
    "    (\"Frequency\", overlap_freq, TopN, overlap_freq/TopN),\n",
    "    (\"IDF\", overlap_idf, TopN, overlap_idf/TopN),\n",
    "    (\"TF-IDF\", overlap_tfidf, TopN, overlap_tfidf/TopN),\n",
    "    (\"Entropy\", overlap_entropy, TopN, overlap_entropy/TopN),\n",
    "    (\"Info_Content\", overlap_ic, TopN, overlap_ic/TopN),\n",
    "    (\"Info_Gain\", overlap_ig, TopN, overlap_ig/TopN),\n",
    "    (\"KL_Divergence\", overlap_kl, TopN, overlap_kl/TopN)\n",
    "], columns=[\"Measure\", \"Overlap\", \"TopN\", \"Ratio\"])\n",
    "results[\"Ratio %\"] = (results[\"Ratio\"]*100).round(2)\n",
    "\n",
    "csv_path = os.path.join(ART_DIR, \"spec7_alternative_measures.csv\")\n",
    "results.to_csv(csv_path, index=False)\n",
    "display(results)\n",
    "\n",
    "print(\"\\nSaved (relative):\")\n",
    "print(f\"- artifacts/{os.path.basename(csv_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc89254-7b87-47df-9037-22b5159d1571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
